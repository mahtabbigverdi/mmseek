  0%|                                                                     | 0/471 [00:00<?, ?it/s]/gscratch/krishna/mahtab/miniconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 11%|██████▏                                                   | 50/471 [14:00<1:56:07, 16.55s/it]/gscratch/krishna/mahtab/miniconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 11.8718, 'grad_norm': 521.265914695332, 'learning_rate': 1.2e-07, 'epoch': 0.06}
{'loss': 11.8371, 'grad_norm': 515.898711394894, 'learning_rate': 1.9996203070249514e-07, 'epoch': 0.13}
{'loss': 11.7483, 'grad_norm': 235.77649680273583, 'learning_rate': 1.995352071694337e-07, 'epoch': 0.19}
{'loss': 11.4694, 'grad_norm': 324.44292822589887, 'learning_rate': 1.9863613034027222e-07, 'epoch': 0.25}
{'loss': 11.2022, 'grad_norm': 134.947942852022, 'learning_rate': 1.9726906596185638e-07, 'epoch': 0.32}
  warnings.warn(
 21%|████████████▌                                              | 100/471 [24:46<53:48,  8.70s/it]/gscratch/krishna/mahtab/miniconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 10.5947, 'grad_norm': 315.6864863728737, 'learning_rate': 1.9544050018795073e-07, 'epoch': 0.38}
{'loss': 10.4081, 'grad_norm': 209.14941640011756, 'learning_rate': 1.9315910880512788e-07, 'epoch': 0.44}
{'loss': 10.2065, 'grad_norm': 262.97466045455445, 'learning_rate': 1.9043571606975774e-07, 'epoch': 0.51}
{'loss': 9.7031, 'grad_norm': 342.93285864532606, 'learning_rate': 1.8728324335139813e-07, 'epoch': 0.57}
{'loss': 8.7374, 'grad_norm': 254.13749305732136, 'learning_rate': 1.8371664782625284e-07, 'epoch': 0.63}
  warnings.warn(
 32%|██████████████████▊                                        | 150/471 [32:25<46:34,  8.71s/it]/gscratch/krishna/mahtab/miniconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 8.3, 'grad_norm': 145.23948577719102, 'learning_rate': 1.797528515115709e-07, 'epoch': 0.7}
{'loss': 8.1487, 'grad_norm': 148.83482724155772, 'learning_rate': 1.7541066097768962e-07, 'epoch': 0.76}
{'loss': 7.9533, 'grad_norm': 141.63152293677177, 'learning_rate': 1.7071067811865473e-07, 'epoch': 0.82}
{'loss': 7.8026, 'grad_norm': 103.71844614950986, 'learning_rate': 1.6567520240477343e-07, 'epoch': 0.89}
{'loss': 7.5371, 'grad_norm': 161.5047390441337, 'learning_rate': 1.6032812508087188e-07, 'epoch': 0.95}
  warnings.warn(
 42%|█████████████████████████                                  | 200/471 [40:11<39:25,  8.73s/it]/gscratch/krishna/mahtab/miniconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 7.0452, 'grad_norm': 126.87308268432132, 'learning_rate': 1.546948158122427e-07, 'epoch': 1.01}
{'loss': 6.8937, 'grad_norm': 88.31124340794382, 'learning_rate': 1.488020023160998e-07, 'epoch': 1.08}
{'loss': 6.5418, 'grad_norm': 144.29066823364494, 'learning_rate': 1.4267764354964035e-07, 'epoch': 1.14}
{'loss': 6.3207, 'grad_norm': 89.40051759771441, 'learning_rate': 1.3635079705638297e-07, 'epoch': 1.2}
{'loss': 6.0815, 'grad_norm': 181.77730594334483, 'learning_rate': 1.2985148110016947e-07, 'epoch': 1.27}
  warnings.warn(
 53%|███████████████████████████████▎                           | 250/471 [47:43<32:05,  8.71s/it]/gscratch/krishna/mahtab/miniconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 5.9186, 'grad_norm': 163.08058671139366, 'learning_rate': 1.2321053224094678e-07, 'epoch': 1.33}
{'loss': 5.7181, 'grad_norm': 134.28574421657973, 'learning_rate': 1.1645945902807339e-07, 'epoch': 1.39}
{'loss': 5.607, 'grad_norm': 129.7226327655008, 'learning_rate': 1.0963029250531416e-07, 'epoch': 1.46}
{'loss': 5.5021, 'grad_norm': 159.68786650212576, 'learning_rate': 1.027554342368162e-07, 'epoch': 1.52}
{'loss': 5.3421, 'grad_norm': 94.78786825698145, 'learning_rate': 9.586750257511866e-08, 'epoch': 1.58}
  warnings.warn(
 64%|█████████████████████████████████████▌                     | 300/471 [55:13<24:47,  8.70s/it]/gscratch/krishna/mahtab/miniconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 5.2812, 'grad_norm': 169.5185275281642, 'learning_rate': 8.899917790059207e-08, 'epoch': 1.65}
{'loss': 5.204, 'grad_norm': 94.23389105961046, 'learning_rate': 8.218304756658071e-08, 'epoch': 1.71}
{'loss': 5.1512, 'grad_norm': 118.8648889865861, 'learning_rate': 7.545145128592009e-08, 'epoch': 1.77}
{'loss': 5.1095, 'grad_norm': 47.19250185832219, 'learning_rate': 6.883632769240588e-08, 'epoch': 1.84}
{'loss': 5.0325, 'grad_norm': 59.89937035532976, 'learning_rate': 6.236906280521646e-08, 'epoch': 1.9}
  warnings.warn(
 74%|██████████████████████████████████████████▎              | 350/471 [1:03:00<17:37,  8.74s/it]/gscratch/krishna/mahtab/miniconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 4.9874, 'grad_norm': 57.732503796993626, 'learning_rate': 5.6080341115262976e-08, 'epoch': 1.96}
{'loss': 4.8557, 'grad_norm': 88.30393557918661, 'learning_rate': 5.000000000000002e-08, 'epoch': 2.03}
{'loss': 4.9281, 'grad_norm': 76.62612492205814, 'learning_rate': 4.415688815743858e-08, 'epoch': 2.09}
{'loss': 4.9249, 'grad_norm': 132.75549018894478, 'learning_rate': 3.857872873103322e-08, 'epoch': 2.15}
{'loss': 4.858, 'grad_norm': 71.48745075600331, 'learning_rate': 3.329198777485869e-08, 'epoch': 2.22}
  warnings.warn(
 85%|████████████████████████████████████████████████▍        | 400/471 [1:10:42<10:21,  8.76s/it]/gscratch/krishna/mahtab/miniconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 4.8488, 'grad_norm': 54.901711365675006, 'learning_rate': 2.832174868315489e-08, 'epoch': 2.28}
{'loss': 4.863, 'grad_norm': 69.12445706013538, 'learning_rate': 2.3691593180019364e-08, 'epoch': 2.34}
{'loss': 4.8521, 'grad_norm': 161.09184356698785, 'learning_rate': 1.9423489433902184e-08, 'epoch': 2.41}
{'loss': 4.8137, 'grad_norm': 119.27495415372269, 'learning_rate': 1.553768782775351e-08, 'epoch': 2.47}
{'loss': 4.7944, 'grad_norm': 153.73179017781058, 'learning_rate': 1.2052624879351104e-08, 'epoch': 2.53}
  warnings.warn(
 96%|██████████████████████████████████████████████████████▍  | 450/471 [1:18:15<03:03,  8.71s/it]/gscratch/krishna/mahtab/miniconda3/envs/qwen2vl/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 4.7575, 'grad_norm': 139.69838566479376, 'learning_rate': 8.98483576766631e-09, 'epoch': 2.6}
{'loss': 4.7924, 'grad_norm': 65.41390949015525, 'learning_rate': 6.348875880294535e-09, 'epoch': 2.66}
{'loss': 4.7758, 'grad_norm': 90.23168769011644, 'learning_rate': 4.157251754174729e-09, 'epoch': 2.72}
{'loss': 4.7663, 'grad_norm': 65.0729550635257, 'learning_rate': 2.420361737256438e-09, 'epoch': 2.79}
{'loss': 4.7495, 'grad_norm': 91.52742934917072, 'learning_rate': 1.146446652649169e-09, 'epoch': 2.85}
  warnings.warn(
100%|█████████████████████████████████████████████████████████| 471/471 [1:21:47<00:00, 10.42s/it]
{'loss': 4.7325, 'grad_norm': 102.832018086164, 'learning_rate': 3.4155069933301526e-10, 'epoch': 2.91}
{'loss': 4.7767, 'grad_norm': 108.16366889156038, 'learning_rate': 9.49277494008971e-12, 'epoch': 2.98}
{'train_runtime': 4909.9121, 'train_samples_per_second': 12.32, 'train_steps_per_second': 0.096, 'train_loss': 6.726511434891168, 'epoch': 2.98}
Saving updated tokenizer...
